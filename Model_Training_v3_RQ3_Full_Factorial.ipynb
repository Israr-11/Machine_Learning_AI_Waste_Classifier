{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1hgZm6KPVqV"
   },
   "source": [
    "**Full Factorial vs. Isolation Approach: Overview**\n",
    "\n",
    "The full factorial design approach used in this notebook differs significantly from an isolation approach in several key ways:\n",
    "\n",
    "**Full Factorial Approach**\n",
    "\n",
    "**Tests all combinations**: Instead of testing one factor at a time, the full\n",
    "factorial design tests all possible combinations of architecture, filter size, depth, and regularization.\n",
    "\n",
    "**Reveals interaction effects**: By testing all combinations, we can discover how factors interact with each other. For example, we might find that a specific architecture works particularly well with a certain filter size, which wouldn't be apparent when testing in isolation.\n",
    "\n",
    "**Finds the true optimum**: The factorial approach is more likely to find the globally optimal configuration because it explores the entire parameter space rather than optimizing each parameter independently.\n",
    "\n",
    "**Statistical analysis**: Enables formal analysis of main effects and interactions, providing quantitative evidence of each factor's impact.\n",
    "\n",
    "**Comprehensive but resource-intensive**: Requires more computational resources as the number of combinations grows exponentially with the number of factors and levels.\n",
    "\n",
    "**Isolation Approach (Used in Previous Experiments)**\n",
    "\n",
    "**Tests one factor at a time**: Changes only one parameter while keeping others constant.\n",
    "\n",
    "**Misses interactions**: Cannot detect how different factors might work together or against each other.\n",
    "\n",
    "**May find local optima**: By optimizing each parameter separately, you might end up with a suboptimal configuration.\n",
    "\n",
    "**More efficient**: Requires fewer experiments, making it less computationally expensive.\n",
    "\n",
    "**Simpler analysis**: Results are easier to interpret but provide less insight into complex relationships.\n",
    "\n",
    "The full factorial approach in this notebook provides a more thorough understanding of how different CNN configurations affect waste classification performance, allowing us to answer both research questions with greater confidence and detail. It reveals not just which individual settings work best, but how they work together as a complete system.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsVh5geCAfCE"
   },
   "source": [
    "## Step 1: Importing the necessary libaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 40012,
     "status": "ok",
     "timestamp": 1752492091858,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "D4n9ncysZLjR",
    "outputId": "5d0804b5-8d12-481b-c24a-820a9abb252d"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57832,
     "status": "ok",
     "timestamp": 1752492215228,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "jYIM1lBooqLT",
    "outputId": "98c2f8e1-d39a-4433-cad8-52bae2fe368c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from IPython.display import display, Markdown\n",
    "from google.colab import drive\n",
    "\n",
    "# MOUNT GOOGLE DRIVE\n",
    "drive.mount('/content/drive')\n",
    "Files_save_path = '/content/drive/MyDrive/<path>'\n",
    "os.makedirs(Files_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIt1oXAJkk9K"
   },
   "source": [
    "## Step 2: Load the Dataset into TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25811,
     "status": "ok",
     "timestamp": 1752492296861,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "Ea93ZYMck82s",
    "outputId": "39c7b96f-54d5-4036-e44d-c43616b08a13"
   },
   "outputs": [],
   "source": [
    "# PATH TO THE DATASET\n",
    "dataset_path = '/content/drive/MyDrive/<path to dataset>'\n",
    "\n",
    "# TRAINING DATASET V2 (80% OF DATA)\n",
    "train_ds = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# VALIDATION DATASET V2 (20% OF DATA)\n",
    "val_ds = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=34\n",
    ")\n",
    "\n",
    "# CLASS NAMES\n",
    "class_names = train_ds.class_names\n",
    "print(f\"Class Names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q64hrhzK4Re2"
   },
   "source": [
    "## Step 3: Visualizing the images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "executionInfo": {
     "elapsed": 2088,
     "status": "ok",
     "timestamp": 1752492340966,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "XnNuvg7S4hSt",
    "outputId": "2d1fd71c-f123-477e-b191-e66002c7377a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):  # IT WILL TAKE ONE BATCH\n",
    "    for i in range(9):  # SHOW 9 IMAGES\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))  # CONVERT TO PROPER FORMAT\n",
    "        plt.title(class_names[labels[i]])  # ADD LABEL\n",
    "        plt.axis(\"off\")\n",
    "    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15ElJRTf8zv3"
   },
   "source": [
    "## Step 4: Normalize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1752492346515,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "u1vp1vwU8_W0"
   },
   "outputs": [],
   "source": [
    "# NORMALIZE PIXEL VALUES (DIVIDE BY 255)\n",
    "train_ds = train_ds.map(lambda x, y: (x / 255.0, y))  # X REPRESENTS IMAGE DATA, Y REPRESENTS LABELS\n",
    "val_ds = val_ds.map(lambda x, y: (x / 255.0, y))\n",
    "\n",
    "# CONFIGURING THE DATASET FOR PERFORMANCE\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcSUwV7oCk3T"
   },
   "source": [
    "## Step 5: Defining the Full Factorial Experiment Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPNUu8O_MqzH"
   },
   "source": [
    "**Purpose**: Set up all possible combinations of CNN configurations to test\n",
    "\n",
    "**Why**: This systematic approach ensures we find the optimal configuration by testing all interactions between factors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1752492438327,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "-SEzHcL-CpUU",
    "outputId": "b007e946-19f5-433e-c827-7c6e472fea47"
   },
   "outputs": [],
   "source": [
    "# DEFINING THE FACTORS AND THEIR LEVELS\n",
    "architectures = ['Simple', 'VGG', 'ResNet', 'MobileNet']\n",
    "filter_sizes = [(3, 3), (5, 5), (7, 7)]\n",
    "depths = [2, 3, 4]  # REDUCED FROM [2,3,4,5] TO LIMIT TOTAL COMBINATIONS\n",
    "regularizations = ['None', 'Dropout', 'L2', 'BatchNorm']\n",
    "\n",
    "# CREATING ALL POSSIBLE COMBINATIONS\n",
    "all_combinations = list(product(architectures, filter_sizes, depths, regularizations))\n",
    "print(f\"Total number of combinations to test: {len(all_combinations)}\")\n",
    "print(f\"First 5 combinations: {all_combinations[:5]}\")\n",
    "\n",
    "# CREATING A DIRECTORY TO SAVE MODELS\n",
    "models_save_path = '/content/drive/MyDrive/<path>'\n",
    "os.makedirs(models_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z9yEZ56GxSM"
   },
   "source": [
    "## Step 6: Creating the Model Factory Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6RhGx0gM1Wd"
   },
   "source": [
    "**Purpose**: Generate CNN models based on specific combinations of architecture, filter size, depth, and regularization\n",
    "\n",
    "**Why**: This modular approach allows us to systematically test all parameter combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1752492441327,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "lOlOcd5HG4Dx"
   },
   "outputs": [],
   "source": [
    "def create_model_with_combination(architecture, filter_size, depth, regularization):\n",
    "    \"\"\"CREATE A MODEL WITH THE SPECIFIED COMBINATION OF FACTORS\"\"\"\n",
    "\n",
    "    # BASE PARAMETERS\n",
    "    reg_value = 0.2 if regularization == 'Dropout' else 0.001 if regularization == 'L2' else None\n",
    "\n",
    "    #1.  SIMPLE CNN IMPLEMENTATION\n",
    "    if architecture == 'Simple':\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        # INPUT LAYER\n",
    "        kernel_reg = tf.keras.regularizers.l2(reg_value) if regularization == 'L2' else None\n",
    "        model.add(tf.keras.layers.Conv2D(32, filter_size, activation='relu',\n",
    "                                         kernel_regularizer=kernel_reg,\n",
    "                                         input_shape=(128, 128, 3)))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        # APPLY REGULARIZATION IF SPECIFIED\n",
    "        if regularization == 'Dropout':\n",
    "            model.add(tf.keras.layers.Dropout(reg_value))\n",
    "        elif regularization == 'BatchNorm':\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        # ADD ADDITIONAL LAYERS BASED ON DEPTH\n",
    "        filters = 64\n",
    "        for _ in range(depth - 1):  # -1 BECAUSE WE ALREADY ADDED ONE CONV BLOCK\n",
    "            model.add(tf.keras.layers.Conv2D(filters, filter_size, activation='relu',\n",
    "                                            kernel_regularizer=kernel_reg))\n",
    "            model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "            # APPLY REGULARIZATION IF SPECIFIED\n",
    "            if regularization == 'Dropout':\n",
    "                model.add(tf.keras.layers.Dropout(reg_value))\n",
    "            elif regularization == 'BatchNorm':\n",
    "                model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "            filters = min(filters * 2, 512)  # DOUBLE FILTERS UP TO 512\n",
    "\n",
    "        # CLASSIFIER\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu',\n",
    "                                       kernel_regularizer=kernel_reg))\n",
    "\n",
    "        # APPLY REGULARIZATION IF SPECIFIED\n",
    "        if regularization == 'Dropout':\n",
    "            model.add(tf.keras.layers.Dropout(reg_value))\n",
    "        elif regularization == 'BatchNorm':\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "\n",
    "    #2. VGG-STYLE IMPLEMENTATION\n",
    "    elif architecture == 'VGG':\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        # FIRST BLOCK\n",
    "        kernel_reg = tf.keras.regularizers.l2(reg_value) if regularization == 'L2' else None\n",
    "        model.add(tf.keras.layers.Conv2D(64, filter_size, padding='same', activation='relu',\n",
    "                                        kernel_regularizer=kernel_reg,\n",
    "                                        input_shape=(128, 128, 3)))\n",
    "\n",
    "        # ADD MORE BLOCKS BASED ON DEPTH\n",
    "        filters = 64\n",
    "        for i in range(depth):\n",
    "            # SECOND CONV IN THE BLOCK\n",
    "            model.add(tf.keras.layers.Conv2D(filters, filter_size, padding='same',\n",
    "                                           activation='relu',\n",
    "                                           kernel_regularizer=kernel_reg))\n",
    "\n",
    "            # APPLY REGULARIZATION IF SPECIFIED\n",
    "            if regularization == 'BatchNorm':\n",
    "                model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "            model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "            # APPLY DROPOUT IF SPECIFIED\n",
    "            if regularization == 'Dropout':\n",
    "                model.add(tf.keras.layers.Dropout(reg_value))\n",
    "\n",
    "            # INCREASE FILTERS FOR NEXT BLOCK\n",
    "            filters = min(filters * 2, 512)\n",
    "\n",
    "        # CLASSIFIER\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(512, activation='relu',\n",
    "                                      kernel_regularizer=kernel_reg))\n",
    "\n",
    "        # APPLY REGULARIZATION IF SPECIFIED\n",
    "        if regularization == 'Dropout':\n",
    "            model.add(tf.keras.layers.Dropout(reg_value))\n",
    "        elif regularization == 'BatchNorm':\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "\n",
    "# 3. RESNET-STYLE IMPLEMENTATION\n",
    "    elif architecture == 'ResNet':\n",
    "        inputs = tf.keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "        # FIRST CONV LAYER\n",
    "        kernel_reg = tf.keras.regularizers.l2(reg_value) if regularization == 'L2' else None\n",
    "        x = tf.keras.layers.Conv2D(64, filter_size, strides=2, padding='same',\n",
    "                                 kernel_regularizer=kernel_reg)(inputs)\n",
    "\n",
    "        if regularization == 'BatchNorm':\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        x = tf.keras.layers.MaxPooling2D((3, 3), strides=2, padding='same')(x)\n",
    "\n",
    "        # ADD RESIDUAL BLOCKS BASED ON DEPTH\n",
    "        filters = 64\n",
    "        for i in range(depth):\n",
    "            # RESIDUAL BLOCK\n",
    "            shortcut = x\n",
    "\n",
    "            # FIRST CONV IN BLOCK\n",
    "            x = tf.keras.layers.Conv2D(filters, filter_size, padding='same',\n",
    "                                     kernel_regularizer=kernel_reg)(x)\n",
    "\n",
    "            if regularization == 'BatchNorm':\n",
    "                x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "            x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "            # SECOND CONV IN BLOCK\n",
    "            x = tf.keras.layers.Conv2D(filters, filter_size, padding='same',\n",
    "                                     kernel_regularizer=kernel_reg)(x)\n",
    "\n",
    "            if regularization == 'BatchNorm':\n",
    "                x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "            # HANDLE SHORTCUT FOR DIMENSION CHANGES\n",
    "            if i > 0:  # FOR BLOCKS AFTER THE FIRST ONE\n",
    "                shortcut = tf.keras.layers.Conv2D(filters, (1, 1), strides=1, padding='same',\n",
    "                                               kernel_regularizer=kernel_reg)(shortcut)\n",
    "                if regularization == 'BatchNorm':\n",
    "                    shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "\n",
    "            # ADD THE SHORTCUT TO THE MAIN PATH\n",
    "            x = tf.keras.layers.add([x, shortcut])\n",
    "            x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "            if regularization == 'Dropout':\n",
    "                x = tf.keras.layers.Dropout(reg_value)(x)\n",
    "\n",
    "            # INCREASE FILTERS FOR NEXT BLOCK\n",
    "            filters = min(filters * 2, 512)\n",
    "\n",
    "        # GLOBAL AVERAGE POOLING AND CLASSIFIER\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        if regularization == 'Dropout':\n",
    "            x = tf.keras.layers.Dropout(reg_value)(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(len(class_names), activation='softmax',\n",
    "                                kernel_regularizer=kernel_reg)(x)\n",
    "\n",
    "        model = tf.keras.Model(inputs, x)\n",
    "\n",
    "\n",
    "# 4.MOBILENET-STYLE IMPLEMENTATION\n",
    "    elif architecture == 'MobileNet':\n",
    "        def depthwise_separable_conv(x, filters, stride=1):\n",
    "            kernel_reg = tf.keras.regularizers.l2(reg_value) if regularization == 'L2' else None\n",
    "\n",
    "            x = tf.keras.layers.DepthwiseConv2D(\n",
    "                kernel_size=filter_size, strides=stride, padding='same',\n",
    "                kernel_regularizer=kernel_reg)(x)\n",
    "\n",
    "            if regularization == 'BatchNorm':\n",
    "                x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "            x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "            x = tf.keras.layers.Conv2D(filters, kernel_size=1, padding='same',\n",
    "                                     kernel_regularizer=kernel_reg)(x)\n",
    "\n",
    "            if regularization == 'BatchNorm':\n",
    "                x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "            x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "            if regularization == 'Dropout':\n",
    "                x = tf.keras.layers.Dropout(reg_value)(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "        inputs = tf.keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "        # FIRST CONV LAYER\n",
    "        kernel_reg = tf.keras.regularizers.l2(reg_value) if regularization == 'L2' else None\n",
    "        x = tf.keras.layers.Conv2D(32, filter_size, strides=2, padding='same',\n",
    "                                 kernel_regularizer=kernel_reg)(inputs)\n",
    "\n",
    "        if regularization == 'BatchNorm':\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "        filters = 64\n",
    "        for i in range(depth):\n",
    "            stride = 2 if i > 0 else 1  # USE STRIDE 2 FOR ALL BLOCKS EXCEPT THE FIRST\n",
    "            x = depthwise_separable_conv(x, filters, stride=stride)\n",
    "            filters = min(filters * 2, 512)  # DOUBLE FILTERS UP TO 512\n",
    "\n",
    "        # GLOBAL AVERAGE POOLING AND CLASSIFIER\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        if regularization == 'Dropout':\n",
    "            x = tf.keras.layers.Dropout(reg_value)(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(len(class_names), activation='softmax',\n",
    "                                kernel_regularizer=kernel_reg)(x)\n",
    "\n",
    "        model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUgn8IgjFG5s"
   },
   "source": [
    "## Step 7: Defining Training and Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8BVR2_SM7zd"
   },
   "source": [
    "**Purpose**: Create a standardized function to train and evaluate each model configuration\n",
    "\n",
    "**Why**: Ensures consistent training procedures across all models, isolating architecture effects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1752492447865,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "ykNqwfVeFNbE"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_combination(combination, epochs=15):\n",
    "    \"\"\"TRAINING AND EVALUATING A MODEL WITH THE SPECIFIED COMBINATION OF FACTORS\"\"\"\n",
    "    architecture, filter_size, depth, regularization = combination\n",
    "\n",
    "    # CREATE A DESCRIPTIVE NAME FOR THIS COMBINATION\n",
    "    model_name = f\"{architecture}-Filter{filter_size[0]}x{filter_size[1]}-Depth{depth}-{regularization}\"\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "\n",
    "    # CREATE THE MODEL\n",
    "    model = create_model_with_combination(architecture, filter_size, depth, regularization)\n",
    "\n",
    "    # COMPILE THE MODEL\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # DISPLAY MODEL SUMMARY\n",
    "    model.summary()\n",
    "\n",
    "    # TRAIN THE MODEL\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=5, restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # EVALUATE ON VALIDATION SET\n",
    "    val_loss, val_acc = model.evaluate(val_ds)\n",
    "    print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # SAVE RESULTS\n",
    "    results = {\n",
    "        'combination': combination,\n",
    "        'model_name': model_name,\n",
    "        'architecture': architecture,\n",
    "        'filter_size': f\"{filter_size[0]}x{filter_size[1]}\",\n",
    "        'depth': depth,\n",
    "        'regularization': regularization,\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'history': history.history,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "    # SAVE MODEL\n",
    "    model_filename = f'waste_model_{model_name.lower().replace(\" \", \"_\").replace(\"-\", \"_\")}.keras'\n",
    "    model_path = os.path.join(models_save_path, model_filename)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ4w9zwIFnp-"
   },
   "source": [
    "## Step 8: Running the Full Factorial Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK672OLlNEB9"
   },
   "source": [
    "**Purpose**: Execute the experiment with all combinations of parameters\n",
    "\n",
    "**Why**: This comprehensive approach reveals interactions between factors that individual tests would miss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13441197,
     "status": "ok",
     "timestamp": 1752506089096,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "88paqRwPsN2K",
    "outputId": "0f52f891-6bb6-4160-ac95-23582af6d1d4"
   },
   "outputs": [],
   "source": [
    "def run_full_factorial_experiment(combinations, max_combinations=None):\n",
    "    \"\"\"RUN THE FULL FACTORIAL EXPERIMENT WITH THE SPECIFIED COMBINATIONS\"\"\"\n",
    "    # LIMIT THE NUMBER OF COMBINATIONS IF SPECIFIED\n",
    "    if max_combinations is not None and max_combinations < len(combinations):\n",
    "        print(f\"Limiting to {max_combinations} combinations out of {len(combinations)}\")\n",
    "        combinations = combinations[:max_combinations]\n",
    "\n",
    "    # STORE RESULTS\n",
    "    all_results = []\n",
    "\n",
    "    # TRAIN AND EVALUATE EACH COMBINATION\n",
    "    for i, combination in enumerate(combinations):\n",
    "        print(f\"\\nCombination {i+1}/{len(combinations)}\")\n",
    "        result = train_and_evaluate_combination(combination)\n",
    "        all_results.append(result)\n",
    "\n",
    "        # CLEAR MEMORY\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    return all_results\n",
    "\n",
    "max_combinations = 100  # ADJUSTMENT BASED ON COMPUTATIONAL RESOURCES AND TIME CONSTRAINTS\n",
    "factorial_results = run_full_factorial_experiment(all_combinations, max_combinations)\n",
    "\n",
    "# SAVE THE RESULTS TO DISK FOR LATER ANALYSIS\n",
    "import pickle\n",
    "with open(os.path.join(Files_save_path, 'factorial_results.pkl'), 'wb') as f:\n",
    "    # REMOVE THE ACTUAL MODEL OBJECTS BEFORE SAVING TO REDUCE FILE SIZE\n",
    "    results_to_save = [{k: v for k, v in r.items() if k != 'model'} for r in factorial_results]\n",
    "    pickle.dump(results_to_save, f)\n",
    "\n",
    "print(f\"Results saved to {os.path.join(Files_save_path, 'factorial_results.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4GiXwi4GBlD"
   },
   "source": [
    "# Step 9: Analyze Results and Find the Best Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYsaJCuYNGy9"
   },
   "source": [
    "**Purpose**: Process experimental results to identify optimal CNN configurations\n",
    "\n",
    "**Why**: Helps answer research question 3a by finding the best overall configuration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1752506089520,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "G6GA-srFGFoF",
    "outputId": "f9b641d3-b486-473b-aedb-7cb52219a18b"
   },
   "outputs": [],
   "source": [
    "def analyze_factorial_results(results):\n",
    "    \"\"\"ANALYZE THE RESULTS OF THE FACTORIAL EXPERIMENT\"\"\"\n",
    "    # CREATE A DATAFRAME FOR EASIER ANALYSIS\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Model Name': r['model_name'],\n",
    "            'Architecture': r['architecture'],\n",
    "            'Filter Size': r['filter_size'],\n",
    "            'Depth': r['depth'],\n",
    "            'Regularization': r['regularization'],\n",
    "            'Validation Accuracy': r['val_accuracy'],\n",
    "            'Validation Loss': r['val_loss'],\n",
    "            'Training Accuracy': max(r['history']['accuracy']) if 'accuracy' in r['history'] else 0,\n",
    "            'Accuracy Gap': max(r['history']['accuracy']) - r['val_accuracy'] if 'accuracy' in r['history'] else 0\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "\n",
    "    # SORT BY VALIDATION ACCURACY\n",
    "    results_df = results_df.sort_values('Validation Accuracy', ascending=False)\n",
    "\n",
    "    # DISPLAY THE TOP 10 CONFIGURATIONS\n",
    "    print(\"Top 10 Configurations by Validation Accuracy:\")\n",
    "    display(results_df.head(10))\n",
    "\n",
    "    # FIND THE BEST CONFIGURATION\n",
    "    best_config = results_df.iloc[0]\n",
    "    print(f\"\\nBest Configuration:\")\n",
    "    print(f\"Model: {best_config['Model Name']}\")\n",
    "    print(f\"Validation Accuracy: {best_config['Validation Accuracy']:.4f}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# ANALYZE THE RESULTS\n",
    "results_df = analyze_factorial_results(factorial_results)\n",
    "\n",
    "# SAVE THE RESULTS DATAFRAME TO CSV\n",
    "results_df.to_csv(os.path.join(Files_save_path, 'factorial_results_summary.csv'), index=False)\n",
    "print(f\"Results summary saved to {os.path.join(Files_save_path, 'factorial_results_summary.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d19VkqidG5rM"
   },
   "source": [
    "# Step 10: Visualizing the Impact of Different Factors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Mi__IbKNNQd"
   },
   "source": [
    "**Purpose**: Create visualizations to understand how each factor affects model performance\n",
    "\n",
    "**Why**: Helps answer research question 3b by showing the impact of different configurations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4423,
     "status": "ok",
     "timestamp": 1752506094363,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "xrCKfAgpHGFd",
    "outputId": "c5c446e4-cc3b-47e7-c8ba-e3471f5e021b"
   },
   "outputs": [],
   "source": [
    "# 1. IMPACT OF ARCHITECTURE\n",
    "plt.figure(figsize=(12, 6))\n",
    "arch_data = results_df.groupby('Architecture')['Validation Accuracy'].mean().reset_index()\n",
    "arch_data = arch_data.sort_values('Validation Accuracy', ascending=False)\n",
    "\n",
    "plt.bar(arch_data['Architecture'], arch_data['Validation Accuracy'], color='royalblue')\n",
    "plt.title('Average Validation Accuracy by Architecture')\n",
    "plt.xlabel('Architecture')\n",
    "plt.ylabel('Average Validation Accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# ADD VALUE LABELS\n",
    "for i, v in enumerate(arch_data['Validation Accuracy']):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'architecture_impact.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'architecture_impact.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 2. IMPACT OF FILTER SIZE\n",
    "plt.figure(figsize=(12, 6))\n",
    "filter_data = results_df.groupby('Filter Size')['Validation Accuracy'].mean().reset_index()\n",
    "filter_data = filter_data.sort_values('Validation Accuracy', ascending=False)\n",
    "\n",
    "plt.bar(filter_data['Filter Size'], filter_data['Validation Accuracy'], color='forestgreen')\n",
    "plt.title('Average Validation Accuracy by Filter Size')\n",
    "plt.xlabel('Filter Size')\n",
    "plt.ylabel('Average Validation Accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# ADD VALUE LABELS\n",
    "for i, v in enumerate(filter_data['Validation Accuracy']):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'filter_size_impact.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'filter_size_impact.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 3. IMPACT OF NETWORK DEPTH\n",
    "plt.figure(figsize=(12, 6))\n",
    "depth_data = results_df.groupby('Depth')['Validation Accuracy'].mean().reset_index()\n",
    "depth_data = depth_data.sort_values('Depth')  # SORT BY DEPTH FOR BETTER VISUALIZATION\n",
    "\n",
    "plt.bar(depth_data['Depth'].astype(str), depth_data['Validation Accuracy'], color='darkorange')\n",
    "plt.title('Average Validation Accuracy by Network Depth')\n",
    "plt.xlabel('Network Depth')\n",
    "plt.ylabel('Average Validation Accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# ADD VALUE LABELS\n",
    "for i, v in enumerate(depth_data['Validation Accuracy']):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'depth_impact.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'depth_impact.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 4. IMPACT OF REGULARIZATION\n",
    "plt.figure(figsize=(12, 6))\n",
    "reg_data = results_df.groupby('Regularization')['Validation Accuracy'].mean().reset_index()\n",
    "reg_data = reg_data.sort_values('Validation Accuracy', ascending=False)\n",
    "\n",
    "plt.bar(reg_data['Regularization'], reg_data['Validation Accuracy'], color='firebrick')\n",
    "plt.title('Average Validation Accuracy by Regularization')\n",
    "plt.xlabel('Regularization')\n",
    "plt.ylabel('Average Validation Accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# ADD VALUE LABELS\n",
    "for i, v in enumerate(reg_data['Validation Accuracy']):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'regularization_impact.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'regularization_impact.pdf'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H58EOmzSHc20"
   },
   "source": [
    "# Step 11: Analyzing Interaction Effects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MkgdjNmNULW"
   },
   "source": [
    "**Purpose**: Examine how different factors interact with each other\n",
    "\n",
    "**Why**: Reveals complex relationships between parameters that affect model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2954,
     "status": "ok",
     "timestamp": 1752507288591,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "hDjLJz_WHiFc",
    "outputId": "53d45453-df79-4575-dec4-ad80d5d411ca"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# CREATING A HEATMAP OF ARCHITECTURE VS. REGULARIZATION\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap_data = results_df.pivot_table(\n",
    "    values='Validation Accuracy',\n",
    "    index='Architecture',\n",
    "    columns='Regularization',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='viridis', fmt='.3f', vmin=0.5, vmax=1.0)\n",
    "plt.title('Interaction: Architecture vs. Regularization')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'arch_reg_interaction.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'arch_reg_interaction.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# CREATING A HEATMAP OF ARCHITECTURE VS. FILTER SIZE\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap_data = results_df.pivot_table(\n",
    "    values='Validation Accuracy',\n",
    "    index='Architecture',\n",
    "    columns='Filter Size',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='viridis', fmt='.3f', vmin=0.5, vmax=1.0)\n",
    "plt.title('Interaction: Architecture vs. Filter Size')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'arch_filter_interaction.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'arch_filter_interaction.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# CREATING A HEATMAP OF DEPTH VS. REGULARIZATION\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap_data = results_df.pivot_table(\n",
    "    values='Validation Accuracy',\n",
    "    index='Depth',\n",
    "    columns='Regularization',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='viridis', fmt='.3f', vmin=0.5, vmax=1.0)\n",
    "plt.title('Interaction: Network Depth vs. Regularization')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'depth_reg_interaction.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'depth_reg_interaction.pdf'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFq2z6-jHxx0"
   },
   "source": [
    "# Step 12: Evaluating the Best Model in Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98Z1G9Y2NZ5u"
   },
   "source": [
    "**Purpose**: Perform a comprehensive evaluation of the best model configuration\n",
    "\n",
    "**Why**: Provides detailed performance metrics for the optimal waste classification model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 20022,
     "status": "ok",
     "timestamp": 1752507312491,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "SuVLq-slH6Pz",
    "outputId": "a695cd3b-d2b6-4353-e13a-b65b3191107a"
   },
   "outputs": [],
   "source": [
    "# GET THE BEST MODEL CONFIGURATION\n",
    "best_config = results_df.iloc[0]\n",
    "best_model_name = best_config['Model Name']\n",
    "print(f\"Evaluating best model: {best_model_name}\")\n",
    "\n",
    "# FIND THE BEST MODEL IN OUR RESULTS\n",
    "best_model = None\n",
    "for result in factorial_results:\n",
    "    if result['model_name'] == best_model_name:\n",
    "        best_model = result['model']\n",
    "        break\n",
    "\n",
    "if best_model:\n",
    "    # GENERATE PREDICTIONS FOR VALIDATION SET\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for images, labels in val_ds:\n",
    "        preds = best_model.predict(images)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "    # CREATE CONFUSION MATRIX\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - Best Model ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Files_save_path, 'best_model_confusion_matrix.png'), dpi=300)\n",
    "    plt.savefig(os.path.join(Files_save_path, 'best_model_confusion_matrix.pdf'), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # GENERATE CLASSIFICATION REPORT\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # SAVE CLASSIFICATION REPORT TO FILE\n",
    "    with open(os.path.join(Files_save_path, 'best_model_classification_report.txt'), 'w') as f:\n",
    "        f.write(f\"Best Model: {best_model_name}\\n\\n\")\n",
    "        f.write(report)\n",
    "\n",
    "    # CREATE BAR CHART FOR PRECISION, RECALL, AND F1-SCORE\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(len(class_names)))\n",
    "\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.bar(x - width, precision, width, label='Precision')\n",
    "    plt.bar(x, recall, width, label='Recall')\n",
    "    plt.bar(x + width, f1, width, label='F1-score')\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Classification Metrics - Best Model ({best_model_name})')\n",
    "    plt.xticks(x, class_names, rotation=45, ha='right')\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(Files_save_path, 'best_model_metrics.png'), dpi=300)\n",
    "    plt.savefig(os.path.join(Files_save_path, 'best_model_metrics.pdf'), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # TEST ON SAMPLE IMAGES\n",
    "    print(\"\\nTesting best model on sample images:\")\n",
    "\n",
    "    # FUNCTION TO PREDICT AND VISUALIZE RESULTS\n",
    "    def predict_and_visualize(model, img_path):\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(128, 128))\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = img_array / 255.0  # NORMALIZE\n",
    "        img_array = tf.expand_dims(img_array, 0)  # ADD BATCH DIMENSION\n",
    "\n",
    "        # MAKING PREDICTION\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_class = np.argmax(predictions[0])\n",
    "        confidence = predictions[0][predicted_class]\n",
    "\n",
    "        # DISPLAYING IMAGE WITH PREDICTION\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Predicted: {class_names[predicted_class]}\\nConfidence: {confidence:.2f}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # GETTING THE FILENAME FROM THE PATH\n",
    "        filename = os.path.basename(img_path)\n",
    "\n",
    "        # SAVING THE VISUALIZATION\n",
    "        plt.savefig(os.path.join(Files_save_path, f'prediction_{filename}'), dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        return class_names[predicted_class], confidence\n",
    "\n",
    "    # FINDING ONE SAMPLE IMAGE FROM EACH CLASS\n",
    "    sample_images = []\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        # GETTING THE FIRST IMAGE IN THE DIRECTORY\n",
    "        for file in os.listdir(class_path)[:1]:\n",
    "            if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                sample_images.append(os.path.join(class_path, file))\n",
    "\n",
    "    # PREDICTING ON EACH SAMPLE IMAGE\n",
    "    results = []\n",
    "    for img_path in sample_images:\n",
    "        class_name = os.path.basename(os.path.dirname(img_path))\n",
    "        print(f\"\\nTesting image from class: {class_name}\")\n",
    "        predicted_class, confidence = predict_and_visualize(best_model, img_path)\n",
    "        results.append({\n",
    "            'Image': os.path.basename(img_path),\n",
    "            'True Class': class_name,\n",
    "            'Predicted Class': predicted_class,\n",
    "            'Confidence': confidence,\n",
    "            'Correct': class_name == predicted_class\n",
    "        })\n",
    "\n",
    "    # DISPLAY RESULTS IN A TABLE\n",
    "    results_df = pd.DataFrame(results)\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"Best model not found in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLNpN8rZICi0"
   },
   "source": [
    "# Step 13: Analyzing Main Effects and Interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B4fQfbhNeS2"
   },
   "source": [
    "**Purpose**: Perform statistical analysis of how each factor affects model performance\n",
    "\n",
    "**Why**: Provides quantitative evidence for answering research question 3b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1752507456596,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "zxbbelDHIJTc",
    "outputId": "2a14856f-a293-44f5-ce7e-6eb397b1ff30"
   },
   "outputs": [],
   "source": [
    "print(\"Available variables:\", [var for var in dir() if not var.startswith('_')])\n",
    "\n",
    "if 'factorial_results' in locals():\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Architecture': r['architecture'],\n",
    "            'Filter Size': r['filter_size'],\n",
    "            'Depth': r['depth'],\n",
    "            'Regularization': r['regularization'],\n",
    "            'Validation Accuracy': r['val_accuracy'],\n",
    "            'Validation Loss': r['val_loss']\n",
    "        }\n",
    "        for r in factorial_results\n",
    "    ])\n",
    "\n",
    "    print(\"Created results_df with columns:\", results_df.columns.tolist())\n",
    "    print(results_df.head())\n",
    "\n",
    "# CALCULATING MAIN EFFECTS\n",
    "print(\"Main Effects Analysis:\")\n",
    "print(\"=====================\")\n",
    "\n",
    "# ARCHITECTURE EFFECT\n",
    "arch_effect = results_df.groupby('Architecture')['Validation Accuracy'].agg(['mean', 'std', 'count'])\n",
    "arch_effect = arch_effect.sort_values('mean', ascending=False)\n",
    "print(\"\\nArchitecture Effect:\")\n",
    "print(arch_effect)\n",
    "\n",
    "# FILTER SIZE EFFECT\n",
    "filter_effect = results_df.groupby('Filter Size')['Validation Accuracy'].agg(['mean', 'std', 'count'])\n",
    "filter_effect = filter_effect.sort_values('mean', ascending=False)\n",
    "print(\"\\nFilter Size Effect:\")\n",
    "print(filter_effect)\n",
    "\n",
    "# DEPTH EFFECT\n",
    "depth_effect = results_df.groupby('Depth')['Validation Accuracy'].agg(['mean', 'std', 'count'])\n",
    "depth_effect = depth_effect.sort_values('mean', ascending=False)\n",
    "print(\"\\nDepth Effect:\")\n",
    "print(depth_effect)\n",
    "\n",
    "# REGULARIZATION EFFECT\n",
    "reg_effect = results_df.groupby('Regularization')['Validation Accuracy'].agg(['mean', 'std', 'count'])\n",
    "reg_effect = reg_effect.sort_values('mean', ascending=False)\n",
    "print(\"\\nRegularization Effect:\")\n",
    "print(reg_effect)\n",
    "\n",
    "# CALCULATING TWO-WAY INTERACTION EFFECTS\n",
    "print(\"\\nTwo-way Interaction Effects:\")\n",
    "print(\"===========================\")\n",
    "\n",
    "# ARCHITECTURE X FILTER SIZE\n",
    "arch_filter_effect = results_df.pivot_table(\n",
    "    values='Validation Accuracy',\n",
    "    index='Architecture',\n",
    "    columns='Filter Size',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nArchitecture x Filter Size:\")\n",
    "print(arch_filter_effect)\n",
    "\n",
    "# ARCHITECTURE X DEPTH\n",
    "arch_depth_effect = results_df.pivot_table(\n",
    "    values='Validation Accuracy',\n",
    "    index='Architecture',\n",
    "    columns='Depth',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nArchitecture x Depth:\")\n",
    "\n",
    "print(arch_depth_effect)\n",
    "\n",
    "# ARCHITECTURE X REGULARIZATION\n",
    "arch_reg_effect = results_df.pivot_table(\n",
    "    values='Validation Accuracy',\n",
    "    index='Architecture',\n",
    "    columns='Regularization',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nArchitecture x Regularization:\")\n",
    "print(arch_reg_effect)\n",
    "\n",
    "# SAVE THE ANALYSIS TO A TEXT FILE\n",
    "with open(os.path.join(Files_save_path, 'factorial_analysis.txt'), 'w') as f:\n",
    "    f.write(\"Main Effects Analysis:\\n\")\n",
    "    f.write(\"=====================\\n\\n\")\n",
    "\n",
    "    f.write(\"Architecture Effect:\\n\")\n",
    "    f.write(str(arch_effect) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Filter Size Effect:\\n\")\n",
    "    f.write(str(filter_effect) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Depth Effect:\\n\")\n",
    "    f.write(str(depth_effect) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Regularization Effect:\\n\")\n",
    "    f.write(str(reg_effect) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Two-way Interaction Effects:\\n\")\n",
    "    f.write(\"===========================\\n\\n\")\n",
    "\n",
    "    f.write(\"Architecture x Filter Size:\\n\")\n",
    "    f.write(str(arch_filter_effect) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Architecture x Depth:\\n\")\n",
    "    f.write(str(arch_depth_effect) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Architecture x Regularization:\\n\")\n",
    "    f.write(str(arch_reg_effect) + \"\\n\\n\")\n",
    "\n",
    "print(f\"Analysis saved to {os.path.join(Files_save_path, 'factorial_analysis.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoYy5ZgFI_cN"
   },
   "source": [
    "# Step 14: Generating Conclusions and Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob60LerVNjC_"
   },
   "source": [
    "**Purpose**: Synthesize findings into actionable recommendations for waste classification models\n",
    "\n",
    "**Why**: Directly answers both research questions with evidence-based conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1752507463306,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "sTtSfhs7JHV8",
    "outputId": "a337c58f-44e4-4cfc-d966-79b32924da80"
   },
   "outputs": [],
   "source": [
    "# IDENTIFYING THE BEST CONFIGURATION FOR EACH FACTOR\n",
    "best_arch = arch_effect.index[0]\n",
    "best_filter = filter_effect.index[0]\n",
    "best_depth = depth_effect.index[0]\n",
    "best_reg = reg_effect.index[0]\n",
    "\n",
    "# CREATING A MARKDOWN SUMMARY OF FINDINGS\n",
    "conclusions = f\"\"\"\n",
    "# Full Factorial Analysis: CNN Configurations for Waste Classification\n",
    "\n",
    "## Research Question 3a: Best Configuration\n",
    "The optimal configuration for training a waste classification model using CNNs is:\n",
    "\n",
    "- **Architecture**: {best_arch}\n",
    "- **Filter Size**: {best_filter}\n",
    "- **Network Depth**: {best_depth}\n",
    "- **Regularization**: {best_reg}\n",
    "\n",
    "This configuration achieved a validation accuracy of {best_config['Validation Accuracy']:.4f}.\n",
    "\n",
    "## Research Question 3b: Impact of Different Configurations\n",
    "\n",
    "### Architecture Impact\n",
    "- {best_arch} architecture performed best with an average accuracy of {arch_effect.loc[best_arch, 'mean']:.4f}.\n",
    "- The architecture choice had {'the most' if (arch_effect['mean'].max() - arch_effect['mean'].min()) > (filter_effect['mean'].max() - filter_effect['mean'].min()) and (arch_effect['mean'].max() - arch_effect['mean'].min()) > (depth_effect['mean'].max() - depth_effect['mean'].min()) and (arch_effect['mean'].max() - arch_effect['mean'].min()) > (reg_effect['mean'].max() - reg_effect['mean'].min()) else 'a significant'} impact on model performance.\n",
    "- {best_arch} likely performed best because {'of its skip connections that help with gradient flow' if best_arch == 'ResNet' else 'of its efficient depthwise separable convolutions' if best_arch == 'MobileNet' else 'it provides a good balance of depth and width' if best_arch == 'VGG' else 'of its simplicity and fewer parameters'}.\n",
    "\n",
    "### Filter Size Impact\n",
    "- {best_filter} filters performed best with an average accuracy of {filter_effect.loc[best_filter, 'mean']:.4f}.\n",
    "- {'Smaller filters captured fine details better' if '3x3' in best_filter else 'Medium-sized filters balanced detail and context well' if '5x5' in best_filter else 'Larger filters captured broader patterns effectively'}.\n",
    "- The difference between the best and worst filter size was {filter_effect['mean'].max() - filter_effect['mean'].min():.4f}.\n",
    "\n",
    "### Network Depth Impact\n",
    "- A depth of {best_depth} layers performed best with an average accuracy of {depth_effect.loc[best_depth, 'mean']:.4f}.\n",
    "- {'Deeper networks provided better feature extraction capabilities' if best_depth > 3 else 'Moderate depth provided a good balance between capacity and overfitting' if best_depth == 3 else 'Shallower networks were sufficient for this task'}.\n",
    "- The difference between the best and worst depth was {depth_effect['mean'].max() - depth_effect['mean'].min():.4f}.\n",
    "\n",
    "### Regularization Impact\n",
    "- {best_reg} regularization performed best with an average accuracy of {reg_effect.loc[best_reg, 'mean']:.4f}.\n",
    "- {'Batch normalization helped stabilize training and improved generalization' if best_reg == 'BatchNorm' else 'Dropout effectively prevented overfitting' if best_reg == 'Dropout' else 'L2 regularization constrained weights appropriately' if best_reg == 'L2' else 'The model did not require regularization for this dataset'}.\n",
    "- The difference between the best and worst regularization was {reg_effect['mean'].max() - reg_effect['mean'].min():.4f}.\n",
    "\n",
    "### Interaction Effects\n",
    "- The combination of {best_arch} architecture with {best_filter} filters showed particularly strong performance.\n",
    "- {best_arch} architecture worked especially well with {best_reg} regularization.\n",
    "- Network depth of {best_depth} was most effective when combined with {best_reg} regularization.\n",
    "\n",
    "## Recommendations for Waste Classification Models\n",
    "1. **Use {best_arch} architecture** as the foundation for waste classification models.\n",
    "2. **Implement {best_filter} convolutional filters** for optimal feature extraction.\n",
    "3. **Design networks with {best_depth} convolutional blocks** for the right balance of capacity and efficiency.\n",
    "4. **Apply {best_reg} regularization** to improve generalization.\n",
    "5. **Consider interaction effects** when designing models, as certain combinations of factors work particularly well together.\n",
    "\n",
    "## Practical Applications\n",
    "This optimal configuration can be directly applied to waste classification systems in recycling facilities, smart bins, and waste management applications. The model achieves high accuracy while maintaining reasonable computational requirements.\n",
    "\"\"\"\n",
    "\n",
    "# DISPLAY THE CONCLUSIONS\n",
    "display(Markdown(conclusions))\n",
    "\n",
    "# SAVE THE CONCLUSIONS TO A FILE\n",
    "with open(os.path.join(Files_save_path, 'factorial_conclusions.md'), 'w') as f:\n",
    "    f.write(conclusions)\n",
    "\n",
    "print(f\"Conclusions saved to {os.path.join(Files_save_path, 'factorial_conclusions.md')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTCYmzj2O3su"
   },
   "source": [
    "# Step 15: Creating a Final Optimal Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgkKX31sO8VU"
   },
   "source": [
    "**Purpose**: Implement the best configuration from the factorial experiment\n",
    "\n",
    "**Why**: Provides a ready-to-use model that represents the optimal waste classification solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 787860,
     "status": "ok",
     "timestamp": 1752508266743,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "1DEups02O5jE",
    "outputId": "eac9b816-e81d-42d5-855e-cbc45ed08ec4"
   },
   "outputs": [],
   "source": [
    "# CREATING THE OPTIMAL MODEL BASED ON OUR FINDINGS\n",
    "optimal_architecture = best_arch\n",
    "optimal_filter_size = tuple(map(int, best_filter.split('x')))\n",
    "optimal_depth = best_depth\n",
    "optimal_regularization = best_reg\n",
    "\n",
    "print(f\"Creating optimal model with:\")\n",
    "print(f\"- Architecture: {optimal_architecture}\")\n",
    "print(f\"- Filter Size: {optimal_filter_size}\")\n",
    "print(f\"- Depth: {optimal_depth}\")\n",
    "print(f\"- Regularization: {optimal_regularization}\")\n",
    "\n",
    "# CREATING THE MODEL\n",
    "optimal_model = create_model_with_combination(\n",
    "    optimal_architecture,\n",
    "    optimal_filter_size,\n",
    "    optimal_depth,\n",
    "    optimal_regularization\n",
    ")\n",
    "\n",
    "# COMPILING THE MODEL\n",
    "optimal_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# DISPLAYING MODEL SUMMARY\n",
    "optimal_model.summary()\n",
    "\n",
    "# TRAINING THE FINAL MODEL WITH MORE EPOCHS FOR BEST PERFORMANCE\n",
    "final_history = optimal_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=30,  # MORE EPOCHS FOR FINAL MODEL\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=7, restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# EVALUATING THE FINAL MODEL\n",
    "final_val_loss, final_val_acc = optimal_model.evaluate(val_ds)\n",
    "print(f\"\\nFinal Optimal Model:\")\n",
    "print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Validation Loss: {final_val_loss:.4f}\")\n",
    "\n",
    "# SAVING THE FINAL MODEL\n",
    "final_model_path = os.path.join(Files_save_path, 'optimal_waste_classification_model.keras')\n",
    "optimal_model.save(final_model_path)\n",
    "print(f\"Final optimal model saved to {final_model_path}\")\n",
    "\n",
    "# PLOTTING LEARNING CURVES\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(final_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(final_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Curves - Optimal Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(final_history.history['loss'], label='Training Loss')\n",
    "plt.plot(final_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curves - Optimal Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'optimal_model_learning_curves.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'optimal_model_learning_curves.pdf'), dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyP3r0/4N5okql3SmWvEM6Li",
   "gpuType": "V28",
   "machine_shape": "hm",
   "mount_file_id": "1qScjNOzMOT7GBIM7VTCBL3lazAEKN7Rw",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
