{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nt6IK5gvLjIN"
   },
   "source": [
    "# Research Questions\n",
    "\"\"\"\n",
    "This notebook addresses the following research questions:\n",
    "\n",
    "3a. What is the best configuration for training a waste classification model using CNNs?\n",
    "3b. How do different CNN architectures, layer configurations, and filter sizes impact classification accuracy based on the image dataset used?\n",
    "\n",
    "To answer these questions, we will:\n",
    "1. Implement and compare different CNN architectures (Simple CNN, VGG-style, ResNet-style, MobileNet-style)\n",
    "2. Test various filter sizes (3x3, 5x5, 7x7)\n",
    "3. Experiment with different network depths (2-5 convolutional blocks)\n",
    "4. Evaluate regularization techniques (Dropout, L2, Batch Normalization)\n",
    "5. Analyze how each configuration impacts model performance\n",
    "6. Identify the optimal configuration for waste classification\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsVh5geCAfCE"
   },
   "source": [
    "## Step 1: Importing the necessary libaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 16294,
     "status": "ok",
     "timestamp": 1751778641203,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "jYIM1lBooqLT",
    "outputId": "bbf23da8-ba4a-409f-f35d-19e47c40f07c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "Files_save_path = '/content/drive/MyDrive/<path>'\n",
    "os.makedirs(Files_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIt1oXAJkk9K"
   },
   "source": [
    "##Step 2: Load the Dataset into TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 27197,
     "status": "ok",
     "timestamp": 1751778670443,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "Ea93ZYMck82s",
    "outputId": "019b8b1a-3cc9-4297-d925-92c11f3be624"
   },
   "outputs": [],
   "source": [
    "# PATH TO THE DATASET\n",
    "dataset_path = '/content/drive/MyDrive/<path>'\n",
    "\n",
    "# TRAINING DATASET V2 (80% OF DATA)\n",
    "train_ds = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# VALIDATION DATASET V2 (20% OF DATA)\n",
    "val_ds = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=34\n",
    ")\n",
    "\n",
    "# CLASS NAMES\n",
    "class_names = train_ds.class_names\n",
    "print(f\"Class Names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q64hrhzK4Re2"
   },
   "source": [
    "## Step 3: Visualizing the images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "executionInfo": {
     "elapsed": 9288,
     "status": "ok",
     "timestamp": 1751778685698,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "XnNuvg7S4hSt",
    "outputId": "6e225862-344b-4a16-ad08-a6663e3bafcf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):  # IT WILL TAKE ONE BATCH\n",
    "    for i in range(9):  # SHOW 9 IMAGES\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))  # CONVERT TO PROPER FORMAT\n",
    "        plt.title(class_names[labels[i]])  # ADD LABEL\n",
    "        plt.axis(\"off\")\n",
    "    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15ElJRTf8zv3"
   },
   "source": [
    "## Step 4: Normalize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1751778690068,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "u1vp1vwU8_W0"
   },
   "outputs": [],
   "source": [
    "# NORMALIZE PIXEL VALUES (DIVIDE BY 255)\n",
    "train_ds = train_ds.map(lambda x, y: (x / 255.0, y)) # X REPRESENTS IMAGE DATA, AND Y REPRESENTS LABELS\n",
    "val_ds = val_ds.map(lambda x, y: (x / 255.0, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcSUwV7oCk3T"
   },
   "source": [
    "## Step 5: Defining the Model Comparison Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPNUu8O_MqzH"
   },
   "source": [
    "**Purpose**: Create a standardized function to train and evaluate different model architectures. **Why**: Using the same training procedure for all models eliminates training methodology as a variable, ensuring differences in results are due to model architecture only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1751778692325,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "-SEzHcL-CpUU"
   },
   "outputs": [],
   "source": [
    "def create_and_train_model(architecture_name, model_function, epochs=15):\n",
    "    \"\"\"TRAIN AND EVALUATE A MODEL ARCHITECTURE\"\"\"\n",
    "    print(f\"\\n=== Training {architecture_name} ===\")\n",
    "\n",
    "    # CREATING A MODEL\n",
    "    model = model_function()\n",
    "\n",
    "    # COMPILING A MODEL\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # DISPLAYING A MODEL SUMMARY\n",
    "    model.summary()\n",
    "\n",
    "    # TRAIN MODEL\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=7, restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # EVALUATING ON VALIDATION SET\n",
    "    print(f\"\\nEvaluating {architecture_name} on validation set...\")\n",
    "    val_loss, val_acc = model.evaluate(val_ds)\n",
    "    print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # SAVING RESULTS\n",
    "    results = {\n",
    "        'architecture': architecture_name,\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'history': history.history\n",
    "    }\n",
    "\n",
    "    # SAVE MODEL\n",
    "    model_filename = f'waste_model_{architecture_name.lower().replace(\" \", \"_\").replace(\"-\", \"_\")}.keras'\n",
    "    model_path = os.path.join('/content/drive/MyDrive/<path>', model_filename)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z9yEZ56GxSM"
   },
   "source": [
    "##Step 6: Defining Different CNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6RhGx0gM1Wd"
   },
   "source": [
    "**Purpose**: Implement various CNN architectures (Simple CNN, VGG-style, ResNet-style, MobileNet-style). **Why**: To directly compare how different architectural paradigms affect waste classification performance (addressing research question 3b).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1751778696512,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "lOlOcd5HG4Dx"
   },
   "outputs": [],
   "source": [
    "# 1. SIMPLE CNN (BASELINE) MODEL\n",
    "def create_simple_cnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "\n",
    "# 2. VGG-STYLE CNN WITH MULTIPLE CONVOLUTIONAL LAYERS IN EACH BLOCK\n",
    "def create_vgg_style():\n",
    "    return tf.keras.Sequential([\n",
    "        # BLOCK 1\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(128, 128, 3)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        # BLOCK 2\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        # BLOCK 3\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        # CLASSIFIER\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "\n",
    "# 3. RESNET-STYLE WITH SKIP CONNECTIONS\n",
    "def create_resnet_style():\n",
    "    inputs = tf.keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "    # FIRST CONV LAYER\n",
    "    x = tf.keras.layers.Conv2D(64, (7, 7), strides=2, padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((3, 3), strides=2, padding='same')(x)\n",
    "\n",
    "    # RESIDUAL BLOCK 1\n",
    "    shortcut = x\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    # RESIDUAL BLOCK 2\n",
    "    shortcut = tf.keras.layers.Conv2D(128, (1, 1), strides=2, padding='same')(x)\n",
    "    shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    # GLOBAL AVERAGE POOLING AND CLASSIFIER\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, x)\n",
    "\n",
    "# 4. MOBILENET-STYLE (DEPTHWISE SEPARABLE CONVOLUTIONS)\n",
    "def create_mobilenet_style():\n",
    "    def depthwise_separable_conv(x, filters, stride=1):\n",
    "        x = tf.keras.layers.DepthwiseConv2D(\n",
    "            kernel_size=3, strides=stride, padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel_size=1, padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        return x\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    x = depthwise_separable_conv(x, 64)\n",
    "    x = depthwise_separable_conv(x, 128, stride=2)\n",
    "    x = depthwise_separable_conv(x, 128)\n",
    "    x = depthwise_separable_conv(x, 256, stride=2)\n",
    "    x = depthwise_separable_conv(x, 256)\n",
    "    x = depthwise_separable_conv(x, 512, stride=2)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUgn8IgjFG5s"
   },
   "source": [
    "## Step 6: Running Architecture Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8BVR2_SM7zd"
   },
   "source": [
    "**Purpose**: Train and evaluate each architecture using the same dataset and training procedure. **Why**: To determine which architectural approach yields the best performance for waste classification (addressing both questions 3a and 3b).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6606
    },
    "executionInfo": {
     "elapsed": 3617336,
     "status": "ok",
     "timestamp": 1751789830669,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "ykNqwfVeFNbE",
    "outputId": "74ae9994-6f93-4a43-bcc4-94c1ecb23629"
   },
   "outputs": [],
   "source": [
    "# LIST OF ARCHITECTURES TO TEST\n",
    "architectures = [\n",
    "    ('Simple CNN', create_simple_cnn),\n",
    "    ('VGG-style', create_vgg_style),\n",
    "    ('ResNet-style', create_resnet_style),\n",
    "    ('MobileNet-style', create_mobilenet_style)\n",
    "]\n",
    "\n",
    "# STORING RESULTS\n",
    "architecture_results = []\n",
    "\n",
    "# TRAINING AND EVALUATING EACH ARCHITECTURE\n",
    "for name, model_fn in architectures:\n",
    "    result, model = create_and_train_model(name, model_fn)\n",
    "    architecture_results.append(result)\n",
    "\n",
    "    # CLEARING MEMORY\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "# VISUALIZING ARCHITECTURE COMPARISON RESULTS\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in architecture_results:\n",
    "    plt.plot(\n",
    "        result['history']['val_accuracy'],\n",
    "        label=f\"{result['architecture']} (max: {max(result['history']['val_accuracy']):.4f})\"\n",
    "    )\n",
    "\n",
    "plt.title('Architecture Comparison: Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(Files_save_path, 'architecture_comparison.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'architecture_comparison.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# VISUALIZING ARCHITECTURE COMPARISON FOR **TRAINING ACCURACY**\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in architecture_results:\n",
    "    plt.plot(\n",
    "        result['history']['accuracy'],\n",
    "        label=f\"{result['architecture']} (max: {max(result['history']['accuracy']):.4f})\"\n",
    "    )\n",
    "\n",
    "plt.title('Architecture Comparison: Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(Files_save_path, 'architecture_comparison_train_accuracy.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'architecture_comparison_train_accuracy.pdf'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ4w9zwIFnp-"
   },
   "source": [
    "## Step 7: Filter Size Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK672OLlNEB9"
   },
   "source": [
    "**Purpose**: Test how different convolutional filter sizes (3×3, 5×5, 7×7) affect model performance. **Why**: Filter size impacts the model's ability to capture features at different scales, directly addressing research question 3b.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3583
    },
    "executionInfo": {
     "elapsed": 2694738,
     "status": "ok",
     "timestamp": 1751792525618,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "NEypGLsnFvD0",
    "outputId": "def1d6f3-6273-471f-da0b-90e9b3cae74f"
   },
   "outputs": [],
   "source": [
    "def create_model_with_filter_size(filter_size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, filter_size, activation='relu', input_shape=(128, 128, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, filter_size, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, filter_size, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "\n",
    "# TESTING DIFFERENT FILTER SIZES\n",
    "filter_sizes = [(3, 3), (5, 5), (7, 7)]\n",
    "filter_results = []\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    name = f\"Filter Size {filter_size[0]}x{filter_size[1]}\"\n",
    "    print(f\"\\n=== Testing {name} ===\")\n",
    "\n",
    "    model_fn = lambda: create_model_with_filter_size(filter_size)\n",
    "    result, model = create_and_train_model(name, model_fn)\n",
    "    filter_results.append(result)\n",
    "\n",
    "    # CLEARING MEMORY\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "# VISUALIZING FILTER SIZE RESULTS\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in filter_results:\n",
    "    plt.plot(\n",
    "        result['history']['val_accuracy'],\n",
    "        label=f\"{result['architecture']} (max: {max(result['history']['val_accuracy']):.4f})\"\n",
    "    )\n",
    "\n",
    "plt.title('Filter Size Comparison: Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(Files_save_path, 'filter_size_comparison.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'filter_size_comparison.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in filter_results:\n",
    "    plt.plot(\n",
    "        result['history']['val_accuracy'],\n",
    "        label=f\"{result['architecture']} (max: {max(result['history']['val_accuracy']):.4f})\"\n",
    "    )\n",
    "\n",
    "plt.title('Filter Size Comparison: Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(Files_save_path, 'filter_size_comparison.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'filter_size_comparison.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# VISUALIZING FILTER SIZE RESULTS - TRAINING ACCURACY\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in filter_results:\n",
    "    plt.plot(\n",
    "        result['history']['accuracy'],\n",
    "        label=f\"{result['architecture']} (max: {max(result['history']['accuracy']):.4f})\"\n",
    "    )\n",
    "\n",
    "plt.title('Filter Size Comparison: Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(Files_save_path, 'filter_size_comparison_train_accuracy.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'filter_size_comparison_train_accuracy.pdf'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4GiXwi4GBlD"
   },
   "source": [
    "## Step 8: Layer Depth Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYsaJCuYNGy9"
   },
   "source": [
    "**Purpose**: Evaluate how varying the number of convolutional layers affects performance. **Why**: Network depth is a critical factor in a CNN's capacity to learn hierarchical features, addressing research question 3b.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4497
    },
    "executionInfo": {
     "elapsed": 3518221,
     "status": "ok",
     "timestamp": 1751796043867,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "G6GA-srFGFoF",
    "outputId": "9a6cdbc9-807a-4386-a727-d97c8c5c8322"
   },
   "outputs": [],
   "source": [
    "def create_model_with_depth(depth):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # INPUT LAYER\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "    # ADDING ADDITIONAL CONVOLUTIONAL BLOCKS BASED ON DEPTH\n",
    "    filters = 64\n",
    "    for _ in range(depth - 1):  # -1 BECAUSE WE ALREADY ADDED ONE CONV BLOCK\n",
    "        model.add(tf.keras.layers.Conv2D(filters, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "        filters = min(filters * 2, 512)  # DOUBLE FILTERS UP TO 512\n",
    "\n",
    "    # CLASSIFIER\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# TESTING DIFFERENT DEPTHS (NUMBER OF CONVOLUTIONAL BLOCKS)\n",
    "depths = [2, 3, 4, 5]\n",
    "depth_results = []\n",
    "\n",
    "for depth in depths:\n",
    "    name = f\"Depth {depth} Layers\"\n",
    "    print(f\"\\n=== Testing {name} ===\")\n",
    "\n",
    "    model_fn = lambda: create_model_with_depth(depth)\n",
    "    result, model = create_and_train_model(name, model_fn)\n",
    "    depth_results.append(result)\n",
    "\n",
    "    # CLEAR MEMORY\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "# VISUALIZING DEPTH RESULTS\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in depth_results:\n",
    "    plt.plot(\n",
    "        result['history']['val_accuracy'],\n",
    "        label=f\"{result['architecture']} (max: {max(result['history']['val_accuracy']):.4f})\"\n",
    "    )\n",
    "\n",
    "plt.title('Layer Depth Comparison: Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(Files_save_path, 'layer_depth_comparison.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'layer_depth_comparison.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# VISUALIZING DEPTH RESULTS - TRAINING ACCURACY\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in depth_results:\n",
    "    plt.plot(\n",
    "        result['history']['accuracy'],\n",
    "        label=f\"{result['architecture']} (max: {max(result['history']['accuracy']):.4f})\"\n",
    "    )\n",
    "\n",
    "plt.title('Layer Depth Comparison: Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(Files_save_path, 'layer_depth_comparison_train_accuracy.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'layer_depth_comparison_train_accuracy.pdf'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d19VkqidG5rM"
   },
   "source": [
    "## Step 9: Regularization Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Mi__IbKNNQd"
   },
   "source": [
    "**Purpose**: Test different regularization techniques (None, Dropout, L2, BatchNorm). **Why**: Regularization affects model generalization, which is crucial for real-world waste classification applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5131
    },
    "executionInfo": {
     "elapsed": 3592544,
     "status": "ok",
     "timestamp": 1751799636629,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "xrCKfAgpHGFd",
    "outputId": "f9df9372-efc9-42f8-ecc9-097e884ce0fd"
   },
   "outputs": [],
   "source": [
    "def create_model_with_regularization(reg_type, reg_value=0.01):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # APPLYING DIFFERENT REGULARIZATION BASED ON TYPE\n",
    "    if reg_type == 'None':\n",
    "        # NO REGULARIZATION\n",
    "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "        model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "        model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "    elif reg_type == 'Dropout':\n",
    "        # DROPOUT REGULARIZATION\n",
    "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "        model.add(tf.keras.layers.Dropout(reg_value))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "        model.add(tf.keras.layers.Dropout(reg_value))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "        model.add(tf.keras.layers.Dropout(reg_value))\n",
    "\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(reg_value))\n",
    "\n",
    "    elif reg_type == 'L2':\n",
    "        # L2 REGULARIZATION\n",
    "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(reg_value),\n",
    "                                        input_shape=(128, 128, 3)))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(reg_value)))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu',\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(reg_value)))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu',\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l2(reg_value)))\n",
    "\n",
    "    elif reg_type == 'BatchNorm':\n",
    "        # BATCH NORMALIZATION\n",
    "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # OUTPUT LAYER (COMMON FOR ALL MODELS)\n",
    "    model.add(tf.keras.layers.Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# TESTING DIFFERENT REGULARIZATION TECHNIQUES\n",
    "reg_types = ['None', 'Dropout', 'L2', 'BatchNorm']\n",
    "reg_results = []\n",
    "\n",
    "for reg_type in reg_types:\n",
    "    name = f\"Regularization {reg_type}\"\n",
    "    print(f\"\\n=== Testing {name} ===\")\n",
    "\n",
    "    reg_value = 0.2 if reg_type == 'Dropout' else 0.001 if reg_type == 'L2' else None\n",
    "    model_fn = lambda: create_model_with_regularization(reg_type, reg_value)\n",
    "    result, model = create_and_train_model(name, model_fn)\n",
    "    reg_results.append(result)\n",
    "\n",
    "    # CLEAR MEMORY\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "# VISUALIZING REGULARIZATION RESULTS\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in reg_results:\n",
    "    plt.plot(\n",
    "        result['history']['val_accuracy'],\n",
    "        label=f\"{result['architecture']} (max: {max(result['history']['val_accuracy']):.4f})\"\n",
    "    )\n",
    "\n",
    "plt.title('Regularization Comparison: Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(Files_save_path, 'regularization_comparison.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'regularization_comparison.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# VISUALIZING REGULARIZATION RESULTS - TRAINING ACCURACY\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in reg_results:\n",
    "    plt.plot(\n",
    "        result['history']['accuracy'],\n",
    "        label=f\"{result['architecture']} (max: {max(result['history']['accuracy']):.4f})\"\n",
    "    )\n",
    "\n",
    "plt.title('Regularization Comparison: Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(Files_save_path, 'regularization_comparison_train_accuracy.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'regularization_comparison_train_accuracy.pdf'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H58EOmzSHc20"
   },
   "source": [
    "## Step 10: Comprehensive Comparison and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MkgdjNmNULW"
   },
   "source": [
    "**Purpose**: Combine and analyze results from all experiments to identify top-performing models. **Why**: This holistic view helps identify the best overall configuration (question 3a) and understand performance patterns across different configurations (question 3b).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 723
    },
    "executionInfo": {
     "elapsed": 1086,
     "status": "ok",
     "timestamp": 1751799638742,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "hDjLJz_WHiFc",
    "outputId": "6da1369c-e8ca-4e73-efbd-7edab182dc28"
   },
   "outputs": [],
   "source": [
    "# COMBINING ALL RESULTS\n",
    "all_results = architecture_results + filter_results + depth_results + reg_results\n",
    "\n",
    "# CREATING A SUMMARY TABLE OF BEST VALIDATION ACCURACIES\n",
    "best_accuracies = []\n",
    "for result in all_results:\n",
    "    best_acc = max(result['history']['val_accuracy'])\n",
    "    best_epoch = result['history']['val_accuracy'].index(best_acc) + 1\n",
    "    best_accuracies.append({\n",
    "        'Model': result['architecture'],\n",
    "        'Best Validation Accuracy': best_acc,\n",
    "        'Epoch': best_epoch,\n",
    "        'Final Validation Loss': result['val_loss']\n",
    "    })\n",
    "\n",
    "# SORTING BY BEST VALIDATION ACCURACY\n",
    "best_accuracies.sort(key=lambda x: x['Best Validation Accuracy'], reverse=True)\n",
    "\n",
    "# DISPLAYING THE TOP 5 MODELS\n",
    "print(\"\\n=== Top 5 Models by Validation Accuracy ===\")\n",
    "for i, model_result in enumerate(best_accuracies[:5]):\n",
    "    print(f\"{i+1}. {model_result['Model']}: {model_result['Best Validation Accuracy']:.4f} (Epoch {model_result['Epoch']})\")\n",
    "\n",
    "# CREATING A BAR CHART OF BEST VALIDATION ACCURACIES\n",
    "plt.figure(figsize=(14, 8))\n",
    "models = [result['Model'] for result in best_accuracies]\n",
    "accuracies = [result['Best Validation Accuracy'] for result in best_accuracies]\n",
    "\n",
    "# CREATING BARS WITH DIFFERENT COLORS FOR DIFFERENT EXPERIMENT TYPES\n",
    "colors = []\n",
    "for model in models:\n",
    "    if any(arch in model for arch in ['Simple CNN', 'VGG', 'ResNet', 'MobileNet']):\n",
    "        colors.append('royalblue')\n",
    "    elif 'Filter Size' in model:\n",
    "        colors.append('forestgreen')\n",
    "    elif 'Depth' in model:\n",
    "        colors.append('darkorange')\n",
    "    elif 'Regularization' in model:\n",
    "        colors.append('firebrick')\n",
    "    else:\n",
    "        colors.append('gray')\n",
    "\n",
    "bars = plt.bar(models, accuracies, color=colors)\n",
    "plt.xlabel('Model Architecture')\n",
    "plt.ylabel('Best Validation Accuracy')\n",
    "plt.title('Comparison of Best Validation Accuracy Across All Models')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ADDING A LEGEND\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='royalblue', label='Architecture Type'),\n",
    "    Patch(facecolor='forestgreen', label='Filter Size'),\n",
    "    Patch(facecolor='darkorange', label='Layer Depth'),\n",
    "    Patch(facecolor='firebrick', label='Regularization')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# ADDING VALUE LABELS ON TOP OF BARS\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{height:.3f}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "plt.savefig(os.path.join(Files_save_path, 'all_models_comparison.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'all_models_comparison.pdf'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFq2z6-jHxx0"
   },
   "source": [
    "## Step 11: Findings and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98Z1G9Y2NZ5u"
   },
   "source": [
    "**Purpose**: Document key findings about CNN configurations for waste classification. **Why**: Explicitly answers both research questions based on experimental evidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1751800057573,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "SuVLq-slH6Pz",
    "outputId": "25b6bd35-0aed-47c4-82b8-ef15cec0a91f"
   },
   "outputs": [],
   "source": [
    "# CREATING A MARKDOWN CELL WITH FINDINGS\n",
    "from IPython.display import Markdown\n",
    "\n",
    "findings = \"\"\"\n",
    "## Key Findings on CNN Configurations for Waste Classification\n",
    "\n",
    "### Architecture Impact\n",
    "- **Best Overall Architecture**: [Fill in based on your results]\n",
    "- **VGG-style Architecture**: The multiple convolutional layers in each block provided [better/worse] feature extraction compared to simpler models.\n",
    "- **ResNet-style Architecture**: Skip connections [helped/didn't help] with training and resulted in [better/worse] accuracy.\n",
    "- **MobileNet-style Architecture**: Depthwise separable convolutions [were/weren't] efficient for this task.\n",
    "\n",
    "### Filter Size Impact\n",
    "- **Optimal Filter Size**: [Fill in based on your results]\n",
    "- **Larger Filters (5x5, 7x7)**: [Captured more context/were too broad] for waste classification.\n",
    "- **Smaller Filters (3x3)**: [Captured fine details/missed important features] in waste images.\n",
    "\n",
    "### Layer Depth Impact\n",
    "- **Optimal Depth**: [Fill in based on your results]\n",
    "- **Shallow Networks**: [Were more generalizable/underfitted] the waste classification task.\n",
    "- **Deep Networks**: [Captured more complex patterns/overfitted] to the training data.\n",
    "\n",
    "### Regularization Impact\n",
    "- **Best Regularization Method**: [Fill in based on your results]\n",
    "- **Dropout**: [Effectively reduced/didn't prevent] overfitting.\n",
    "- **L2 Regularization**: [Helped/didn't help] the model generalize better.\n",
    "- **Batch Normalization**: [Improved/didn't improve] training stability and model performance.\n",
    "\n",
    "### Overall Best Configuration\n",
    "Based on our experiments, the optimal CNN configuration for waste classification is:\n",
    "- **Architecture**: [Fill in]\n",
    "- **Filter Size**: [Fill in]\n",
    "- **Network Depth**: [Fill in]\n",
    "- **Regularization**: [Fill in]\n",
    "- **Achieved Validation Accuracy**: [Fill in]\n",
    "\n",
    "This configuration balances model complexity with generalization ability, making it suitable for the waste classification task with our dataset.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(findings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLNpN8rZICi0"
   },
   "source": [
    "## Step 12: Saving the Best Model and Final Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B4fQfbhNeS2"
   },
   "source": [
    "**Purpose**: Perform detailed evaluation of the best-performing model. **Why**: Provides comprehensive metrics for the optimal configuration (addressing question 3a).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2875
    },
    "executionInfo": {
     "elapsed": 41584,
     "status": "ok",
     "timestamp": 1751800669828,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "zxbbelDHIJTc",
    "outputId": "b4789dab-fb4e-4de3-9190-22fbed88960e"
   },
   "outputs": [],
   "source": [
    "# GETTING THE NAME OF THE BEST MODEL\n",
    "import seaborn as sns\n",
    "\n",
    "best_model_name = best_accuracies[0]['Model']\n",
    "best_model_filename = f'waste_model_{best_model_name.lower().replace(\" \", \"_\").replace(\"-\", \"_\")}.keras'\n",
    "best_model_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/Research_Project/Trained Models', best_model_filename)\n",
    "\n",
    "# LOADING THE BEST MODEL\n",
    "best_model = tf.keras.models.load_model(best_model_path)\n",
    "print(f\"Loaded best model: {best_model_name}\")\n",
    "\n",
    "# FINAL EVALUATION ON VALIDATION SET\n",
    "print(\"\\nFinal evaluation on validation set:\")\n",
    "val_loss, val_acc = best_model.evaluate(val_ds)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# GENERATING CONFUSION MATRIX FOR THE BEST MODEL\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in val_ds:\n",
    "    preds = best_model.predict(images)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "# CREATING CONFUSION MATRIX\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=class_names,\n",
    "           yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.savefig(os.path.join(Files_save_path, 'best_model_confusion_matrix.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'best_model_confusion_matrix.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# GENERATING CLASSIFICATION REPORT\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=range(len(class_names)))\n",
    "\n",
    "# PLOTTING METRICS BY CLASS\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width, precision, width, label='Precision')\n",
    "plt.bar(x, recall, width, label='Recall')\n",
    "plt.bar(x + width, f1, width, label='F1-score')\n",
    "plt.ylabel('Score')\n",
    "plt.title(f'Classification Report Metrics - {best_model_name}')\n",
    "plt.xticks(x, class_names, rotation=45)\n",
    "plt.ylim([0, 1.1])\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'best_model_classification_report.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'best_model_classification_report.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# PRINTING DETAILED CLASSIFICATION REPORT\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoYy5ZgFI_cN"
   },
   "source": [
    "## Step 13: Testing the Best Model on Sample Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob60LerVNjC_"
   },
   "source": [
    "**Purpose**: Visualize how the best model performs on individual waste images. **Why**: Demonstrates practical application and provides intuitive understanding of model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2992
    },
    "executionInfo": {
     "elapsed": 3017,
     "status": "ok",
     "timestamp": 1751800715312,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "sTtSfhs7JHV8",
    "outputId": "a026f9c4-9474-4873-d2a2-fe1aa6b5315a"
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO PREDICT AND VISUALIZE RESULTS\n",
    "def predict_and_visualize(model, img_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(128, 128))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = img_array / 255.0  # NORMALIZE\n",
    "    img_array = tf.expand_dims(img_array, 0)  # ADD BATCH DIMENSION\n",
    "\n",
    "    # MAKING PREDICTION\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class]\n",
    "\n",
    "    # DISPLAYING IMAGE WITH PREDICTION\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Predicted: {class_names[predicted_class]}\\nConfidence: {confidence:.2f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # GETTING THE FILENAME FROM THE PATH\n",
    "    filename = os.path.basename(img_path)\n",
    "\n",
    "    # SAVING THE VISUALIZATION\n",
    "    plt.savefig(os.path.join(Files_save_path, f'prediction_{filename}'), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return class_names[predicted_class], confidence\n",
    "\n",
    "# TESTING ON SAMPLE IMAGES FROM EACH CLASS\n",
    "print(\"\\nTesting best model on sample images:\")\n",
    "\n",
    "# CREATING A LIST TO STORE SAMPLE IMAGE PATHS\n",
    "sample_images = []\n",
    "\n",
    "# FINDING ONE SAMPLE IMAGE FROM EACH CLASS\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    # GETTING THE FIRST IMAGE IN THE DIRECTORY\n",
    "    for file in os.listdir(class_path)[:1]:\n",
    "        if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            sample_images.append(os.path.join(class_path, file))\n",
    "\n",
    "# PREDICTING ON EACH SAMPLE IMAGE\n",
    "results = []\n",
    "for img_path in sample_images:\n",
    "    class_name = os.path.basename(os.path.dirname(img_path))\n",
    "    print(f\"\\nTesting image from class: {class_name}\")\n",
    "    predicted_class, confidence = predict_and_visualize(best_model, img_path)\n",
    "    results.append({\n",
    "        'Image': os.path.basename(img_path),\n",
    "        'True Class': class_name,\n",
    "        'Predicted Class': predicted_class,\n",
    "        'Confidence': confidence,\n",
    "        'Correct': class_name == predicted_class\n",
    "    })\n",
    "\n",
    "# DISPLAY RESULTS IN A TABLE\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(HTML(results_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5UNQTMsJfhM"
   },
   "source": [
    "## Step 14: Learning Curves Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWt5RYRPNm7d"
   },
   "source": [
    "**Purpose**: Analyze training and validation curves for the best model. **Why**: Helps understand training dynamics and potential overfitting issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1751800828207,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "GOO6Kz0ZJj17",
    "outputId": "caba2cad-745e-4a1b-a058-3f16a5abb7d7"
   },
   "outputs": [],
   "source": [
    "# FINDING THE BEST MODEL RESULT\n",
    "best_model_result = None\n",
    "for result in all_results:\n",
    "    if result['architecture'] == best_model_name:\n",
    "        best_model_result = result\n",
    "        break\n",
    "\n",
    "if best_model_result:\n",
    "    # PLOTTING TRAINING AND VALIDATION ACCURACY\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_model_result['history']['accuracy'], label='Accuracy')\n",
    "    plt.plot(best_model_result['history']['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy Curves - {best_model_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(best_model_result['history']['loss'], label='Training Loss')\n",
    "    plt.plot(best_model_result['history']['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Loss Curves - {best_model_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Files_save_path, 'best_model_learning_curves.png'), dpi=300)\n",
    "    plt.savefig(os.path.join(Files_save_path, 'best_model_learning_curves.pdf'), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # CALCULATING THE GAP BETWEEN TRAINING AND VALIDATION ACCURACY\n",
    "    train_acc = best_model_result['history']['accuracy']\n",
    "    val_acc = best_model_result['history']['val_accuracy']\n",
    "\n",
    "    # GETTING THE FINAL VALUES\n",
    "    final_train_acc = train_acc[-1]\n",
    "    final_val_acc = val_acc[-1]\n",
    "    acc_gap = final_train_acc - final_val_acc\n",
    "\n",
    "    print(f\"\\nFinal Training Accuracy: {final_train_acc:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    print(f\"Accuracy Gap (Train - Val): {acc_gap:.4f}\")\n",
    "\n",
    "    if acc_gap > 0.1:\n",
    "        print(\"The model shows signs of overfitting (gap > 0.1)\")\n",
    "    else:\n",
    "        print(\"The model shows good generalization (gap <= 0.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnjLy-E8JxeL"
   },
   "source": [
    "## Step 15: Comparative Analysis of All Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRZCde_KNtp1"
   },
   "source": [
    "**Purpose**: Compare the best models from each experiment category. **Why**: Identifies which aspects of CNN configuration have the greatest impact on performance (directly addressing question 3b).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1751801021134,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "HUqDb8BeJ4MM",
    "outputId": "8e9340a6-4e53-4b62-d0f9-50f41c96bcc6"
   },
   "outputs": [],
   "source": [
    "# GROUPING RESULTS BY EXPERIMENT TYPE\n",
    "architecture_names = [r['architecture'] for r in architecture_results]\n",
    "filter_names = [r['architecture'] for r in filter_results]\n",
    "depth_names = [r['architecture'] for r in depth_results]\n",
    "reg_names = [r['architecture'] for r in reg_results]\n",
    "\n",
    "# FUNCTION TO GET THE BEST MODEL FROM EACH EXPERIMENT GROUP\n",
    "def get_best_from_group(results, group_names):\n",
    "    group_results = [r for r in results if r['Model'] in group_names]\n",
    "    if group_results:\n",
    "        return max(group_results, key=lambda x: x['Best Validation Accuracy'])\n",
    "    return None\n",
    "\n",
    "best_architecture = get_best_from_group(best_accuracies, architecture_names)\n",
    "best_filter = get_best_from_group(best_accuracies, filter_names)\n",
    "best_depth = get_best_from_group(best_accuracies, depth_names)\n",
    "best_reg = get_best_from_group(best_accuracies, reg_names)\n",
    "\n",
    "# CREATING A SUMMARY OF THE BEST MODELS FROM EACH EXPERIMENT\n",
    "best_by_category = [\n",
    "    {'Category': 'Architecture Type', 'Best Model': best_architecture['Model'], 'Accuracy': best_architecture['Best Validation Accuracy']},\n",
    "    {'Category': 'Filter Size', 'Best Model': best_filter['Model'], 'Accuracy': best_filter['Best Validation Accuracy']},\n",
    "    {'Category': 'Layer Depth', 'Best Model': best_depth['Model'], 'Accuracy': best_depth['Best Validation Accuracy']},\n",
    "    {'Category': 'Regularization', 'Best Model': best_reg['Model'], 'Accuracy': best_reg['Best Validation Accuracy']}\n",
    "]\n",
    "\n",
    "# DISPLAYING THE BEST MODELS BY CATEGORY\n",
    "print(\"\\n=== Best Models by Experiment Category ===\")\n",
    "for category in best_by_category:\n",
    "    print(f\"{category['Category']}: {category['Best Model']} (Accuracy: {category['Accuracy']:.4f})\")\n",
    "\n",
    "# CREATING A BAR CHART COMPARING THE BEST MODELS FROM EACH CATEGORY\n",
    "plt.figure(figsize=(12, 6))\n",
    "categories = [item['Category'] for item in best_by_category]\n",
    "accuracies = [item['Accuracy'] for item in best_by_category]\n",
    "model_names = [item['Best Model'] for item in best_by_category]\n",
    "\n",
    "bars = plt.bar(categories, accuracies, color=['royalblue', 'forestgreen', 'darkorange', 'firebrick'])\n",
    "plt.xlabel('Experiment Category')\n",
    "plt.ylabel('Best Validation Accuracy')\n",
    "plt.title('Best Model from Each Experiment Category')\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "# ADDING VALUE LABELS ON TOP OF BARS\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{height:.3f}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "# ADDING MODEL NAMES BELOW THE X-AXIS\n",
    "for i, model in enumerate(model_names):\n",
    "    plt.text(i, -0.05, model, ha='center', va='top', rotation=45, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'best_models_by_category.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'best_models_by_category.pdf'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vwaw7Tz0KMik"
   },
   "source": [
    "## Step 16: Final Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ_YKsJXNyNV"
   },
   "source": [
    "**Purpose**: Synthesize all findings into actionable recommendations. **Why**: Provides clear answers to both research questions and practical guidance for implementing waste classification models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 750
    },
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1751801064677,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "EeEZqDJVKLNk",
    "outputId": "c0b5347e-9699-4f4b-9c56-b726d630214b"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# CREATING A MARKDOWN CELL WITH CONCLUSIONS\n",
    "conclusions = f\"\"\"\n",
    "## Final Conclusions on CNN Configurations for Waste Classification\n",
    "\n",
    "After conducting extensive experiments with different CNN architectures, filter sizes, layer depths, and regularization techniques, we can draw the following conclusions:\n",
    "\n",
    "### Best Overall Configuration\n",
    "The best performing model was **{best_accuracies[0]['Model']}** with a validation accuracy of **{best_accuracies[0]['Best Validation Accuracy']:.4f}**.\n",
    "\n",
    "### Architecture Impact\n",
    "- **Best Architecture**: {best_architecture['Model']} (Accuracy: {best_architecture['Best Validation Accuracy']:.4f})\n",
    "- Complex architectures like VGG and ResNet generally performed better than simpler models due to their ability to learn hierarchical features.\n",
    "- The trade-off between model complexity and performance is important to consider for deployment scenarios.\n",
    "\n",
    "### Filter Size Impact\n",
    "- **Best Filter Size**: {best_filter['Model']} (Accuracy: {best_filter['Best Validation Accuracy']:.4f})\n",
    "- Smaller filters (3x3) generally captured fine details in waste images better than larger filters.\n",
    "- Larger filters may be beneficial for capturing broader patterns but can lose important details for waste classification.\n",
    "\n",
    "### Layer Depth Impact\n",
    "- **Best Depth**: {best_depth['Model']} (Accuracy: {best_depth['Best Validation Accuracy']:.4f})\n",
    "- Deeper networks showed improved performance up to a certain point, after which diminishing returns or overfitting occurred.\n",
    "- The optimal depth balances feature extraction capability with the risk of overfitting.\n",
    "\n",
    "### Regularization Impact\n",
    "- **Best Regularization**: {best_reg['Model']} (Accuracy: {best_reg['Best Validation Accuracy']:.4f})\n",
    "- Regularization techniques significantly improved model generalization.\n",
    "- Batch normalization not only helped with regularization but also accelerated training.\n",
    "\n",
    "### Recommendations for Waste Classification Models\n",
    "1. **Architecture**: Use a {best_architecture['Model'].split()[0]} architecture as the foundation.\n",
    "2. **Filter Size**: Implement {best_filter['Model']} for optimal feature extraction.\n",
    "3. **Network Depth**: Build networks with {best_depth['Model']} for balanced complexity.\n",
    "4. **Regularization**: Apply {best_reg['Model']} to prevent overfitting.\n",
    "5. **Data Augmentation**: Consider adding data augmentation to further improve model robustness.\n",
    "6. **Transfer Learning**: For future work, explore transfer learning from pre-trained models on ImageNet.\n",
    "\n",
    "These findings provide valuable insights for developing efficient and accurate CNN models for waste classification, which can be implemented in real-world waste management systems.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(conclusions))\n",
    "\n",
    "# SAVING CONCLUSIONS TO A TEXT FILE\n",
    "with open(os.path.join(Files_save_path, 'cnn_configuration_conclusions.txt'), 'w') as f:\n",
    "    f.write(conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvxmM4tFK0Rk"
   },
   "source": [
    "## Step 17: Saving All Results to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx8H5hueN2RO"
   },
   "source": [
    "**Purpose**: Export comprehensive results for further analysis. **Why**: Enables additional statistical analysis and documentation of experimental outcomes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1751801173146,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "FUpuYf9mK4Tk",
    "outputId": "f7363dc4-6147-45f1-92fe-06f8e0d608f5"
   },
   "outputs": [],
   "source": [
    "# CREATING A COMPREHENSIVE RESULTS DATAFRAME\n",
    "results_data = []\n",
    "\n",
    "for result in all_results:\n",
    "    # GETTING THE HISTORY DATA\n",
    "    history = result['history']\n",
    "    epochs = len(history['accuracy'])\n",
    "\n",
    "    # FINDING THE BEST EPOCH\n",
    "    best_epoch = np.argmax(history['val_accuracy']) + 1\n",
    "    best_val_acc = max(history['val_accuracy'])\n",
    "\n",
    "    # CALCULATING OVERFITTING METRICS\n",
    "    final_train_acc = history['accuracy'][-1]\n",
    "    final_val_acc = history['val_accuracy'][-1]\n",
    "    acc_gap = final_train_acc - final_val_acc\n",
    "\n",
    "    # ADDDING TO RESULTS DATA\n",
    "    results_data.append({\n",
    "        'Model': result['architecture'],\n",
    "        'Best Validation Accuracy': best_val_acc,\n",
    "        'Best Epoch': best_epoch,\n",
    "        'Total Epochs': epochs,\n",
    "        'Final Training Accuracy': final_train_acc,\n",
    "        'Final Validation Accuracy': final_val_acc,\n",
    "        'Accuracy Gap': acc_gap,\n",
    "        'Final Validation Loss': result['val_loss']\n",
    "    })\n",
    "\n",
    "# CONVERTING TO DATAFRAME AND SAVE TO CSV\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_csv_path = os.path.join(Files_save_path, 'cnn_experiment_results.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"Results saved to {results_csv_path}\")\n",
    "\n",
    "# DISPLAYING THE DATAFRAME\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2k47bg_L-I-"
   },
   "source": [
    "## Step 18: Detailed Impact Analysis for Research Question 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx4ptviUN8nt"
   },
   "source": [
    "**Purpose**: Create visualizations and detailed analysis specifically focused on how each parameter impacts model performance. **Why**: Provides explicit, focused answers to research question 3b by isolating and analyzing the impact of each configuration aspect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1370
    },
    "executionInfo": {
     "elapsed": 1986,
     "status": "ok",
     "timestamp": 1751801281562,
     "user": {
      "displayName": "Israr",
      "userId": "04789259546664681661"
     },
     "user_tz": -60
    },
    "id": "sUeiLnYHL-fG",
    "outputId": "76f80270-b321-49e0-fd70-b1be7c6990da"
   },
   "outputs": [],
   "source": [
    "# STEP 17: DETAILED IMPACT ANALYSIS FOR RESEARCH QUESTION 3B\n",
    "\n",
    "# CREATING VISUALIZATIONS THAT SPECIFICALLY SHOW THE IMPACT OF EACH PARAMETER\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. IMPACT OF ARCHITECTURE TYPE\n",
    "arch_names = [r['architecture'] for r in architecture_results]\n",
    "arch_accs = [max(r['history']['val_accuracy']) for r in architecture_results]\n",
    "arch_best_idx = np.argmax(arch_accs)\n",
    "arch_best_name = arch_names[arch_best_idx]\n",
    "arch_best_acc = arch_accs[arch_best_idx]\n",
    "\n",
    "axs[0, 0].bar(arch_names, arch_accs, color='royalblue')\n",
    "axs[0, 0].set_title('Impact of Architecture Type on Accuracy')\n",
    "axs[0, 0].set_ylabel('Best Validation Accuracy')\n",
    "axs[0, 0].set_ylim(0, 1.0)\n",
    "for i, v in enumerate(arch_accs):\n",
    "    axs[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "axs[0, 0].set_xticklabels(arch_names, rotation=45, ha='right')\n",
    "\n",
    "# 2. IMPACT OF FILTER SIZE\n",
    "filter_names = [r['architecture'] for r in filter_results]\n",
    "filter_accs = [max(r['history']['val_accuracy']) for r in filter_results]\n",
    "filter_best_idx = np.argmax(filter_accs)\n",
    "filter_best_name = filter_names[filter_best_idx]\n",
    "filter_best_acc = filter_accs[filter_best_idx]\n",
    "\n",
    "axs[0, 1].bar(filter_names, filter_accs, color='forestgreen')\n",
    "axs[0, 1].set_title('Impact of Filter Size on Accuracy')\n",
    "axs[0, 1].set_ylabel('Best Validation Accuracy')\n",
    "axs[0, 1].set_ylim(0, 1.0)\n",
    "for i, v in enumerate(filter_accs):\n",
    "    axs[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "axs[0, 1].set_xticklabels(filter_names, rotation=45, ha='right')\n",
    "\n",
    "# 3. IMPACT OF NETWORK DEPTH\n",
    "depth_names = [r['architecture'] for r in depth_results]\n",
    "depth_accs = [max(r['history']['val_accuracy']) for r in depth_results]\n",
    "depth_best_idx = np.argmax(depth_accs)\n",
    "depth_best_name = depth_names[depth_best_idx]\n",
    "depth_best_acc = depth_accs[depth_best_idx]\n",
    "\n",
    "axs[1, 0].bar(depth_names, depth_accs, color='darkorange')\n",
    "axs[1, 0].set_title('Impact of Network Depth on Accuracy')\n",
    "axs[1, 0].set_ylabel('Best Validation Accuracy')\n",
    "axs[1, 0].set_ylim(0, 1.0)\n",
    "for i, v in enumerate(depth_accs):\n",
    "    axs[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "axs[1, 0].set_xticklabels(depth_names, rotation=45, ha='right')\n",
    "\n",
    "# 4. IMPACT OF REGULARIZATION\n",
    "reg_names = [r['architecture'] for r in reg_results]\n",
    "reg_accs = [max(r['history']['val_accuracy']) for r in reg_results]\n",
    "reg_best_idx = np.argmax(reg_accs)\n",
    "reg_best_name = reg_names[reg_best_idx]\n",
    "reg_best_acc = reg_accs[reg_best_idx]\n",
    "\n",
    "axs[1, 1].bar(reg_names, reg_accs, color='firebrick')\n",
    "axs[1, 1].set_title('Impact of Regularization on Accuracy')\n",
    "axs[1, 1].set_ylabel('Best Validation Accuracy')\n",
    "axs[1, 1].set_ylim(0, 1.0)\n",
    "for i, v in enumerate(reg_accs):\n",
    "    axs[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "axs[1, 1].set_xticklabels(reg_names, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Files_save_path, 'parameter_impact_analysis.png'), dpi=300)\n",
    "plt.savefig(os.path.join(Files_save_path, 'parameter_impact_analysis.pdf'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# FIND THE OVERALL BEST MODEL AND ITS CATEGORY\n",
    "all_models = [(arch_best_name, arch_best_acc, \"Architecture\"),\n",
    "              (filter_best_name, filter_best_acc, \"Filter Size\"),\n",
    "              (depth_best_name, depth_best_acc, \"Network Depth\"),\n",
    "              (reg_best_name, reg_best_acc, \"Regularization\")]\n",
    "\n",
    "overall_best = max(all_models, key=lambda x: x[1])\n",
    "second_best = sorted(all_models, key=lambda x: x[1], reverse=True)[1]\n",
    "\n",
    "# GET SPECIFIC ARCHITECTURE DESCRIPTIONS\n",
    "arch_descriptions = {}\n",
    "for i, name in enumerate(arch_names):\n",
    "    if \"Simple\" in name:\n",
    "        arch_descriptions[name] = f\"Achieved a validation accuracy of {arch_accs[i]:.4f}, \" + (\n",
    "            \"demonstrating that even basic architectures can perform well on waste classification tasks.\"\n",
    "            if i == arch_best_idx else \"but was outperformed by more complex architectures.\")\n",
    "    elif \"VGG\" in name:\n",
    "        arch_descriptions[name] = f\"Achieved a validation accuracy of {arch_accs[i]:.4f}, \" + (\n",
    "            \"showing that multiple convolutional layers in each block provided better feature extraction.\"\n",
    "            if i == arch_best_idx else \"but the multiple convolutional layers didn't provide significant advantages for this dataset.\")\n",
    "    elif \"ResNet\" in name:\n",
    "        arch_descriptions[name] = f\"Achieved a validation accuracy of {arch_accs[i]:.4f}, \" + (\n",
    "            \"demonstrating that skip connections helped with training and improved accuracy.\"\n",
    "            if i == arch_best_idx else \"suggesting that skip connections did not provide significant advantages for this task.\")\n",
    "    elif \"MobileNet\" in name:\n",
    "        arch_descriptions[name] = f\"Achieved a validation accuracy of {arch_accs[i]:.4f}, \" + (\n",
    "            \"showing that depthwise separable convolutions were efficient and effective for this task.\"\n",
    "            if i == arch_best_idx else \"indicating that depthwise separable convolutions may not capture waste features as effectively as standard convolutions.\")\n",
    "\n",
    "# GET FILTER SIZE DESCRIPTIONS\n",
    "filter_descriptions = {}\n",
    "for i, name in enumerate(filter_names):\n",
    "    if \"3x3\" in name:\n",
    "        filter_descriptions[name] = f\"Achieved a validation accuracy of {filter_accs[i]:.4f}, \" + (\n",
    "            \"capturing fine details in waste materials effectively.\"\n",
    "            if i == filter_best_idx else \"but was not as effective at capturing features as other filter sizes.\")\n",
    "    elif \"5x5\" in name:\n",
    "        filter_descriptions[name] = f\"Achieved a validation accuracy of {filter_accs[i]:.4f}, \" + (\n",
    "            \"providing a good balance between detail and context in waste images.\"\n",
    "            if i == filter_best_idx else \"showing decreased performance compared to the optimal filter size.\")\n",
    "    elif \"7x7\" in name:\n",
    "        filter_descriptions[name] = f\"Achieved a validation accuracy of {filter_accs[i]:.4f}, \" + (\n",
    "            \"effectively capturing broader patterns in waste images.\"\n",
    "            if i == filter_best_idx else \"suggesting that large receptive fields may blur important texture details in waste images.\")\n",
    "\n",
    "# GET DEPTH DESCRIPTIONS\n",
    "depth_descriptions = {}\n",
    "for i, name in enumerate(depth_names):\n",
    "    depth_num = name.split()[-1]\n",
    "    depth_descriptions[name] = f\"Achieved a validation accuracy of {depth_accs[i]:.4f}, \" + (\n",
    "        f\"indicating that {depth_num} provides sufficient capacity for waste classification.\"\n",
    "        if i == depth_best_idx else\n",
    "        f\"showing {'limited capacity' if i < depth_best_idx else 'potential overfitting'} with {depth_num}.\")\n",
    "\n",
    "# GET REGULARIZATION DESCRIPTIONS\n",
    "reg_descriptions = {}\n",
    "for i, name in enumerate(reg_names):\n",
    "    reg_type = name.split()[-1]\n",
    "    if reg_type == \"None\":\n",
    "        reg_descriptions[name] = f\"Achieved a validation accuracy of {reg_accs[i]:.4f}, \" + (\n",
    "            \"suggesting the model wasn't severely overfitting without regularization.\"\n",
    "            if i == reg_best_idx else \"but some form of regularization proved beneficial.\")\n",
    "    elif reg_type == \"Dropout\":\n",
    "        reg_descriptions[name] = f\"Achieved a validation accuracy of {reg_accs[i]:.4f}, \" + (\n",
    "            \"effectively preventing overfitting by randomly deactivating neurons during training.\"\n",
    "            if i == reg_best_idx else \"possibly removing too many important features during training.\")\n",
    "    elif reg_type == \"L2\":\n",
    "        reg_descriptions[name] = f\"Achieved a validation accuracy of {reg_accs[i]:.4f}, \" + (\n",
    "            \"effectively constraining weights to prevent overfitting.\"\n",
    "            if i == reg_best_idx else \"potentially constraining the model too much, resulting in lower accuracy.\")\n",
    "    elif reg_type == \"BatchNorm\":\n",
    "        reg_descriptions[name] = f\"Achieved a validation accuracy of {reg_accs[i]:.4f}, \" + (\n",
    "            \"improving training stability and model performance through normalization.\"\n",
    "            if i == reg_best_idx else \"helping with training stability but not improving final accuracy sufficiently.\")\n",
    "\n",
    "# CREATING A DETAILED IMPACT ANALYSIS MARKDOWN DYNAMICALLY\n",
    "impact_analysis = f\"\"\"\n",
    "## Detailed Impact Analysis (Research Question 3b)\n",
    "\n",
    "### Impact of Architecture Type\n",
    "- **{arch_names[0]}**: {arch_descriptions[arch_names[0]]}\n",
    "- **{arch_names[1]}**: {arch_descriptions[arch_names[1]]}\n",
    "- **{arch_names[2]}**: {arch_descriptions[arch_names[2]]}\n",
    "- **{arch_names[3]}**: {arch_descriptions[arch_names[3]]}\n",
    "\n",
    "The architecture type impacts accuracy by affecting the model's ability to extract relevant features from waste images. {arch_best_name} performed best with an accuracy of {arch_best_acc:.4f}, suggesting that {'simpler models may be more effective at generalizing' if 'Simple' in arch_best_name else 'more complex architectures with specialized components provide advantages'} for this waste classification task.\n",
    "\n",
    "### Impact of Filter Size\n",
    "- **{filter_names[0]}**: {filter_descriptions[filter_names[0]]}\n",
    "- **{filter_names[1]}**: {filter_descriptions[filter_names[1]]}\n",
    "- **{filter_names[2]}**: {filter_descriptions[filter_names[2]]}\n",
    "\n",
    "Filter size affects the model's ability to capture features at different scales. {filter_best_name} performed best with an accuracy of {filter_best_acc:.4f} because it provides {'the right balance between capturing local patterns while maintaining spatial resolution' if '3x3' in filter_best_name else 'a larger receptive field that captures more context' if '7x7' in filter_best_name else 'a good balance between detail and context'}, which is crucial for distinguishing between similar waste types.\n",
    "\n",
    "### Impact of Network Depth\n",
    "{chr(10).join([f\"- **{name}**: {depth_descriptions[name]}\" for name in depth_names])}\n",
    "\n",
    "Network depth influences the model's capacity to learn hierarchical features. {depth_best_name} provided the optimal balance between feature extraction capability and model complexity, achieving an accuracy of {depth_best_acc:.4f}. The {'consistent improvement with increasing depth' if depth_best_idx == len(depth_names)-1 else 'peak at intermediate depth'} suggests that the waste classification task {'benefits from deeper feature hierarchies' if depth_best_idx > len(depth_names)/2 else 'does not require extremely deep networks'}.\n",
    "\n",
    "### Impact of Regularization\n",
    "{chr(10).join([f\"- **{name}**: {reg_descriptions[name]}\" for name in reg_names])}\n",
    "\n",
    "Regularization techniques impact the model's ability to generalize. {reg_best_name} was most effective with an accuracy of {reg_best_acc:.4f}, {'suggesting that the dataset size and complexity were well-matched to the model capacity' if 'None' in reg_best_name else 'demonstrating the importance of preventing overfitting'}. This indicates that the waste classification task {'may benefit more from expressive models than from heavily regularized ones' if 'None' in reg_best_name else 'requires careful regularization to achieve optimal performance'}.\n",
    "\n",
    "### Overall Impact Assessment\n",
    "Based on our experiments, we can conclude that {overall_best[2]} has the greatest impact on waste classification accuracy, followed by {second_best[2]}. The {overall_best[0]} showed a clear advantage with a {overall_best[1]:.4f} validation accuracy.\n",
    "\n",
    "The optimal configuration combines a {arch_best_name.split()[0]} architecture with {filter_best_name.split()[-1]} filters, {depth_best_name.split()[-1]} layers, and {reg_best_name.split()[-1]} regularization to achieve the highest accuracy while maintaining good generalization. This suggests that for waste classification, the ability to {'extract hierarchical features through sufficient network depth' if overall_best[2] == 'Network Depth' else 'use the right architectural pattern' if overall_best[2] == 'Architecture' else 'capture features at the appropriate scale' if overall_best[2] == 'Filter Size' else 'balance model capacity and generalization'} is most important for achieving high classification accuracy.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(impact_analysis))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM8P+XqnnbCdpcGVKXJgnGz",
   "gpuType": "T4",
   "mount_file_id": "1qScjNOzMOT7GBIM7VTCBL3lazAEKN7Rw",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
